<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GPT Voice Assistant</title>
  <style>
    body { font-family: sans-serif; margin: 30px; }
    #mic-btn { font-size: 20px; padding: 10px 30px; }
    #status { margin-top: 20px; }
    #output { margin-top: 30px; font-size: 18px; white-space: pre-wrap; }
    #audio-player { margin-top: 20px; display: none; }
  </style>
</head>
<body>
  <h2>GPT Voice Assistant (HTTP Stream)</h2>
  <button id="mic-btn">üé§ –ù–∞—á–∞—Ç—å</button>
  <div id="status"></div>
  <div id="output"></div>
  <audio id="audio-player" autoplay></audio>

  <script>
    const OPENAI_PROXY_URL = "https://openai-gpt-proxy.onrender.com/gpt";
    const STREAM_URL = "https://elevenlabs-render-proxy.onrender.com/stream";

    let messages = [];
    let isRunning = false;

    async function recognizeSpeech(timeout = 10000) {
      return new Promise((resolve, reject) => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) return reject("üé§ –ë—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏");

        const recognition = new SpeechRecognition();
        recognition.lang = "ru-RU";
        recognition.interimResults = false;

        const timer = setTimeout(() => {
          recognition.stop();
          reject("‚è±Ô∏è –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ");
        }, timeout);

        recognition.onstart = () => {
          console.log("üéôÔ∏è STT —Å—Ç–∞—Ä—Ç–∞–Ω—É–ª");
          document.getElementById("status").innerText = "üéôÔ∏è –°–ª—É—à–∞—é...";
        };

        recognition.onresult = (event) => {
          clearTimeout(timer);
          recognition.stop();
          const text = event.results[0][0].transcript;
          document.getElementById("status").innerText = "";
          resolve(text);
        };

        recognition.onerror = (err) => {
          clearTimeout(timer);
          recognition.stop();
          console.warn("üõë STT error:", err);
          reject(err.error || "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è");
        };

        recognition.start();
      });
    }

    async function askGPT() {
      const res = await fetch(OPENAI_PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ messages: messages.slice(-10) })
      });
      const data = await res.json();
      return data.choices?.[0]?.message?.content || "‚ùå GPT –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª";
    }

    async function playTTS(text) {
      document.getElementById("status").innerText = "üîä –û–∑–≤—É—á–∫–∞...";
      const res = await fetch(STREAM_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text })
      });

      if (!res.ok) {
        document.getElementById("status").innerText = "‚õî –û—à–∏–±–∫–∞ –æ–∑–≤—É—á–∫–∏";
        console.warn("üõë Fetch error:", await res.text());
        return;
      }

      const audioBlob = new Blob([await res.arrayBuffer()], { type: "audio/mpeg" });
      console.log("üì¶ Audio blob size:", audioBlob.size);

      if (audioBlob.size < 1000) {
        document.getElementById("status").innerText = "‚õî –ü—É—Å—Ç–æ–π –∑–≤—É–∫";
        return;
      }

      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = document.getElementById("audio-player");
      audio.src = audioUrl;
      audio.load();

      try {
        await audio.play();
      } catch (err) {
        console.warn("üéß autoplay error:", err);
        document.getElementById("status").innerText = "‚õî –ù–∞–∂–º–∏ –Ω–∞ —ç–∫—Ä–∞–Ω";
      }

      await new Promise(resolve => {
        audio.onended = resolve;
        audio.onerror = resolve;
      });

      document.getElementById("status").innerText = "";
    }

    async function dialogLoop() {
      if (isRunning) return;
      isRunning = true;

      try {
        while (true) {
          await new Promise(r => setTimeout(r, 600));

          const userText = await recognizeSpeech();
          messages.push({ role: "user", content: userText });
          document.getElementById("output").innerText += `üßë: ${userText}\n`;

          const reply = await askGPT();
          messages.push({ role: "assistant", content: reply });
          document.getElementById("output").innerText += `ü§ñ: ${reply}\n`;

          await playTTS(reply);
        }
      } catch (err) {
        console.warn("üí• –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ:", err);
        document.getElementById("status").innerText = "‚õî –û—à–∏–±–∫–∞: " + err;
        await new Promise(r => setTimeout(r, 1500));
      } finally {
        isRunning = false;
      }
    }

    document.getElementById("mic-btn").onclick = () => {
      messages = [];
      document.getElementById("output").innerText = "";
      dialogLoop();
    };

    window.addEventListener("load", () => {
      // STT –ø—Ä–æ–≥—Ä–µ–≤
      try {
        const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (Recognition) {
          const warmup = new Recognition();
          warmup.lang = "ru-RU";
          warmup.onstart = () => warmup.abort();
          warmup.start();
        }
      } catch (e) {
        console.warn("üßä STT warmup fail:", e);
      }

      // –ü—Ä–æ–≥—Ä–µ–≤ —Å–µ—Ä–≤–µ—Ä–æ–≤
      fetch(OPENAI_PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ messages: [{ role: "user", content: "ping" }] })
      });

      fetch(STREAM_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text: "–¢–µ—Å—Ç" })
      });
    });
  </script>
</body>
</html>
