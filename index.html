<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GPT Voice Assistant + VAD</title>
  <style>
    body { font-family: sans-serif; margin: 30px; }
    #mic-btn { font-size: 20px; padding: 10px 30px; }
    #status { margin-top: 20px; }
    #output { margin-top: 30px; font-size: 18px; white-space: pre-wrap; }
    #audio-player { margin-top: 20px; display: none; }
  </style>
</head>
<body>
  <h2>GPT Voice Assistant (—Å VAD)</h2>
  <button id="mic-btn">üé§ –ù–∞—á–∞—Ç—å</button>
  <div id="status"></div>
  <div id="output"></div>
  <audio id="audio-player" autoplay></audio>

  <script>
    const OPENAI_PROXY_URL = "https://openai-gpt-proxy.onrender.com/gpt";
    const PROXY_URL = "https://elevenlabs-render-proxy.onrender.com/stream";
    const VOICE_ID = "Yko7PKHZNXotIFUBG7I9";

    let messages = [
      { role: "system", content: "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç." }
    ];
    let isRunning = false;

    async function waitForVoice(threshold = 0.05) {
      document.getElementById("status").innerText = "üü¢ –û–∂–∏–¥–∞—é –≥–æ–ª–æ—Å...";
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioCtx.createMediaStreamSource(stream);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);

      const data = new Uint8Array(analyser.fftSize);

      return new Promise((resolve) => {
        const checkVolume = () => {
          analyser.getByteTimeDomainData(data);
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const val = (data[i] - 128) / 128;
            sum += val * val;
          }
          const volume = Math.sqrt(sum / data.length);
          if (volume > threshold) {
            stream.getTracks().forEach(t => t.stop());
            audioCtx.close();
            resolve();
          } else {
            requestAnimationFrame(checkVolume);
          }
        };
        checkVolume();
      });
    }

    async function recognizeSpeech(timeout = 10000) {
      return new Promise((resolve, reject) => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) return reject("SpeechRecognition –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è");

        const recognition = new SpeechRecognition();
        recognition.lang = "ru-RU";
        recognition.interimResults = false;

        const timer = setTimeout(() => {
          recognition.stop();
          reject("‚è±Ô∏è –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ");
        }, timeout);

        recognition.onstart = () => {
          document.getElementById("status").innerText = "üéôÔ∏è –ì–æ–≤–æ—Ä–∏...";
        };

        recognition.onresult = (event) => {
          clearTimeout(timer);
          recognition.stop();
          const text = event.results[0][0].transcript;
          document.getElementById("status").innerText = "";
          resolve(text);
        };

        recognition.onerror = (err) => {
          clearTimeout(timer);
          recognition.stop();
          reject(err.error);
        };

        recognition.start();
      });
    }

    async function askGPT() {
      const res = await fetch(OPENAI_PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ model: "gpt-3.5-turbo", messages })
      });
      const data = await res.json();
      return data.choices?.[0]?.message?.content || "–û—à–∏–±–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT";
    }

    async function playTTS(text) {
      document.getElementById("status").innerText = "üîä –û–∑–≤—É—á–∫–∞...";
      const res = await fetch(PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          text,
          voice_id: VOICE_ID,
          model_id: "eleven_multilingual_v2",
          output_format: "mp3_44100_128"
        })
      });

      const audioBlob = new Blob([await res.arrayBuffer()], { type: "audio/mpeg" });
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = document.getElementById("audio-player");
      audio.src = audioUrl;
      audio.style.display = "block";
      await audio.play();
      await new Promise((resolve) => {
        audio.onended = resolve;
        audio.onerror = resolve;
      });
      document.getElementById("status").innerText = "";
    }

    async function dialogLoop() {
      if (isRunning) return;
      isRunning = true;
      try {
        while (true) {
          await waitForVoice(); // ‚è±Ô∏è –∂–¥—ë–º, –ø–æ–∫–∞ —Ç—ã –Ω–∞—á–Ω—ë—à—å –≥–æ–≤–æ—Ä–∏—Ç—å
          const userText = await recognizeSpeech();
          messages.push({ role: "user", content: userText });
          document.getElementById("output").innerText += `üßë: ${userText}\n`;

          const reply = await askGPT();
          messages.push({ role: "assistant", content: reply });
          document.getElementById("output").innerText += `ü§ñ: ${reply}\n`;

          await playTTS(reply);
        }
      } catch (err) {
        document.getElementById("status").innerText = "‚õî " + err;
      } finally {
        isRunning = false;
      }
    }

    document.getElementById("mic-btn").onclick = () => {
      document.getElementById("output").innerText = "";
      dialogLoop();
    };
  </script>
</body>
</html>
