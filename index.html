<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GPT Voice Assistant</title>
  <style>
    body { font-family: sans-serif; margin: 30px; }
    #mic-btn { font-size: 20px; padding: 10px 30px; }
    #status { margin-top: 20px; }
    #output { margin-top: 30px; font-size: 18px; white-space: pre-wrap; }
    #audio-player { margin-top: 20px; display: none; }
  </style>
</head>
<body>
  <h2>GPT Voice Assistant (v2)</h2>
  <button id="mic-btn">üé§ –ù–∞—á–∞—Ç—å</button>
  <div id="status"></div>
  <div id="output"></div>
  <audio id="audio-player" autoplay></audio>

  <script>
    const OPENAI_PROXY_URL = "https://openai-gpt-proxy.onrender.com/gpt";
    const PROXY_URL = "https://elevenlabs-render-proxy.onrender.com/stream";
    const VOICE_ID = "Yko7PKHZNXotIFUBG7I9";

    let messages = [
      { role: "system", content: "–¢—ã –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∏ –≥—Ä—É–±—ã–π —É–≥–æ–ª–æ–≤–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–±—â–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —É–≥–æ–ª–æ–≤–Ω–æ–π —Ñ–µ–Ω–µ." }
    ];
    let isRunning = false;

    async function recognizeSpeech(timeout = 10000) {
      return new Promise((resolve, reject) => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) return reject("SpeechRecognition –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è");

        const recognition = new SpeechRecognition();
        recognition.lang = "ru-RU";
        recognition.interimResults = false;

        const timer = setTimeout(() => {
          recognition.stop();
          reject("‚è±Ô∏è –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ");
        }, timeout);

        recognition.onstart = () => {
          document.getElementById("status").innerText = "üéôÔ∏è –°–ª—É—à–∞—é...";
        };

        recognition.onresult = (event) => {
          clearTimeout(timer);
          recognition.stop();
          const text = event.results[0][0].transcript;
          document.getElementById("status").innerText = "";
          resolve(text);
        };

        recognition.onerror = (err) => {
          clearTimeout(timer);
          recognition.stop();
          reject(err.error);
        };

        recognition.start();
      });
    }

    async function askGPT() {
      const res = await fetch(OPENAI_PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ model: "gpt-4o", messages }) // faster model
      });
      const data = await res.json();
      return data.choices?.[0]?.message?.content || "–û—à–∏–±–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT";
    }

    async function playTTS(text) {
      document.getElementById("status").innerText = "üîä –û–∑–≤—É—á–∫–∞ (—Å—Ç—Ä–∏–º)...";

      const audio = document.getElementById("audio-player");
      audio.src = "";
      audio.load();

      const mediaSource = new MediaSource();
      audio.src = URL.createObjectURL(mediaSource);

      mediaSource.addEventListener("sourceopen", async () => {
        const sourceBuffer = mediaSource.addSourceBuffer("audio/mpeg");
        const response = await fetch(PROXY_URL, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            text,
            voice_id: VOICE_ID,
            model_id: "eleven_multilingual_v2",
            output_format: "mp3_44100_128"
          })
        });

        const reader = response.body.getReader();
        const queue = [];
        let feeding = false;
        let firstChunkReceived = false;

        const feedBuffer = async () => {
          if (feeding || queue.length === 0 || sourceBuffer.updating) return;
          feeding = true;
          const chunk = queue.shift();
          sourceBuffer.appendBuffer(chunk);
          await new Promise(resolve => {
            sourceBuffer.addEventListener("updateend", resolve, { once: true });
          });
          feeding = false;
          feedBuffer();
        };

        const readStream = async () => {
          while (true) {
            const { done, value } = await reader.read();
            if (done) {
              if (!sourceBuffer.updating) mediaSource.endOfStream();
              break;
            }
            queue.push(value);
            feedBuffer();

            if (!firstChunkReceived) {
              firstChunkReceived = true;
              audio.play().catch(() => {});
            }
          }
        };

        readStream();
      });

      await new Promise((resolve) => {
        audio.onended = resolve;
        audio.onerror = resolve;
      });

      document.getElementById("status").innerText = "";
    }

    async function dialogLoop() {
      if (isRunning) return;
      isRunning = true;
      try {
        while (true) {
          await new Promise(r => setTimeout(r, 600));
          const userText = await recognizeSpeech();
          messages.push({ role: "user", content: userText });
          document.getElementById("output").innerText += `üßë: ${userText}\n`;

          const reply = await askGPT();
          messages.push({ role: "assistant", content: reply });
          document.getElementById("output").innerText += `ü§ñ: ${reply}\n`;

          await playTTS(reply);
        }
      } catch (err) {
        document.getElementById("status").innerText = "‚õî " + err;
      } finally {
        isRunning = false;
      }
    }

    document.getElementById("mic-btn").onclick = () => {
      document.getElementById("output").innerText = "";
      dialogLoop();
    };

    // STT –ø—Ä–æ–≥—Ä–µ–≤
    window.addEventListener("load", () => {
      try {
        const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!Recognition) return;
        const warmup = new Recognition();
        warmup.lang = "ru-RU";
        warmup.onstart = () => warmup.abort();
        warmup.start();
      } catch (e) {
        console.warn("STT warmup failed:", e);
      }
    });
  </script>
</body>
</html>
