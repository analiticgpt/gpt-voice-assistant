<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GPT Voice Assistant</title>
  <style>
    body { font-family: sans-serif; margin: 30px; }
    #mic-btn { font-size: 20px; padding: 10px 30px; }
    #status { margin-top: 20px; }
    #output { margin-top: 30px; font-size: 18px; white-space: pre-wrap; }
    #audio-player { margin-top: 20px; display: none; }
  </style>
</head>
<body>
  <h2>GPT Voice Assistant (HTTP Stream)</h2>
  <button id="mic-btn">üé§ –ù–∞—á–∞—Ç—å</button>
  <div id="status"></div>
  <div id="output"></div>
  <audio id="audio-player" autoplay></audio>

  <script>
    const OPENAI_PROXY_URL = "https://openai-gpt-proxy.onrender.com/gpt";
    const STREAM_URL = "https://elevenlabs-render-proxy.onrender.com/stream";

    let messages = [];
    let step = 0;
    let isRunning = false;

    async function recognizeSpeech(timeout = 10000) {
      return new Promise((resolve, reject) => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) return reject("SpeechRecognition –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è");

        const recognition = new SpeechRecognition();
        recognition.lang = "ru-RU";
        recognition.interimResults = false;

        const timer = setTimeout(() => {
          recognition.stop();
          reject("‚è±Ô∏è –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ");
        }, timeout);

        recognition.onstart = () => document.getElementById("status").innerText = "üéôÔ∏è –°–ª—É—à–∞—é...";
        recognition.onresult = (event) => {
          clearTimeout(timer);
          recognition.stop();
          const text = event.results[0][0].transcript;
          document.getElementById("status").innerText = "";
          resolve(text);
        };
        recognition.onerror = (err) => {
          clearTimeout(timer);
          recognition.stop();
          reject(err.error);
        };

        recognition.start();
      });
    }

    async function askGPT() {
      const res = await fetch(OPENAI_PROXY_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: messages.slice(-10),
          step: step
        })
      });
      const data = await res.json();
      step = data.step || step;
      return data.choices?.[0]?.message?.content || "–û—à–∏–±–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT";
    }

    async function playTTS(text) {
      document.getElementById("status").innerText = "üîä –û–∑–≤—É—á–∫–∞...";

      const res = await fetch(STREAM_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text })
      });

      if (!res.ok) {
        document.getElementById("status").innerText = "‚õî –û—à–∏–±–∫–∞ –æ–∑–≤—É—á–∫–∏";
        console.warn("üõë Fetch error:", await res.text());
        return;
      }

      const audioBlob = new Blob([await res.arrayBuffer()], { type: "audio/mpeg" });
      console.log("üì¶ Audio blob size:", audioBlob.size);

      if (audioBlob.size < 1000) {
        document.getElementById("status").innerText = "‚õî –ü—É—Å—Ç–æ–π –∑–≤—É–∫";
        return;
      }

      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = document.getElementById("audio-player");
      audio.src = audioUrl;
      audio.load();

      try {
        await audio.play();
      } catch (err) {
        console.warn("üéß autoplay error:", err);
        document.getElementById("status").innerText = "‚õî –ù–∞–∂–º–∏ –Ω–∞ —ç–∫—Ä–∞–Ω";
      }

      await new Promise(resolve => {
        audio.onended = resolve;
        audio.onerror = resolve;
      });

      document.getElementById("status").innerText = "";
    }

    async function dialogLoop() {
      if (isRunning) return;
      isRunning = true;

      try {
        while (true) {
          await new Promise(r => setTimeout(r, 600));
          const userText = await recognizeSpeech();
          messages.push({ role: "user", content: userText });
          document.getElementById("output").innerText += `üßë: ${userText}\n`;

          const reply = await askGPT();
          messages.push({ role: "assistant", content: reply });
          document.getElementById("output").innerText += `ü§ñ: ${reply}\n`;

          await playTTS(reply);
        }
      } catch (err) {
        document.getElementById("status").innerText = "‚õî " + err;
      } finally {
        isRunning = false;
      }
    }

    document.getElementById("mic-btn").onclick = () => {
      messages = []; // —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å—ë
      step = 0;
      document.getElementById("output").innerText = "";
      dialogLoop();
    };

    window.addEventListener("load", () => {
      try {
        const Recognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!Recognition) return;
        const warmup = new Recognition();
        warmup.lang = "ru-RU";
        warmup.onstart = () => warmup.abort();
        warmup.start();
      } catch (e) {
        console.warn("STT warmup failed:", e);
      }
    });
  </script>
</body>
</html>
